{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b8f6e9-668c-4c4b-81dd-5fcfb7956d51",
   "metadata": {},
   "source": [
    "# Classification with a vanilla model vs trained model.\n",
    "\n",
    "## Install pre requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064eb8e6-054d-4f9a-bf97-903706a577b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Atze00/MoViNet-pytorch.git\n",
    "!pip install -q av\n",
    "!pip install -q -U aperturedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382ab3d-dea0-4507-bf41-bcea892a6377",
   "metadata": {},
   "source": [
    "## Util functions\n",
    "\n",
    "### Load datasets as clips (of 16 frames), sampled at 5fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4871f56-82ef-4aaa-99cd-91cc9f36db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "import torch\n",
    "\n",
    "def get_common():\n",
    "    \"\"\"\n",
    "    Just common parameters.\n",
    "    Applies to the training and data loading sections.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(97)\n",
    "    num_frames = 16\n",
    "    clip_steps = 2\n",
    "    Bs_Train = 16\n",
    "    Bs_Test = 16\n",
    "\n",
    "    transform = T.Compose([\n",
    "                                    T.Lambda(lambda x: x.permute(3, 0, 1, 2) / 255.),\n",
    "                                    T.Resize((200, 200)),\n",
    "                                    T.RandomHorizontalFlip(),\n",
    "                                    # T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "                                    T.RandomCrop((172, 172))])\n",
    "    transform_test = T.Compose([\n",
    "                                    T.Lambda(lambda x: x.permute(3, 0, 1, 2) / 255.),\n",
    "                                    # T.ToTensor()/255.0,\n",
    "                                    # T.ToTensor(),\n",
    "                                    T.Resize((200, 200)),\n",
    "                                    # T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "                                    T.CenterCrop((172, 172))])\n",
    "    return num_frames, clip_steps, Bs_Train, Bs_Test, transform, transform_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488febf-57e3-44a4-bc83-253be7ce9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AHMDB51 import AHMDB51\n",
    "\n",
    "def get_data_sets():\n",
    "    \"\"\"\n",
    "    Get the datasets from aperturedb.\n",
    "    The data has been ingested previously.\n",
    "    \"\"\"\n",
    "    num_frames, clip_steps, Bs_Train, Bs_Test, transform, transform_test = get_common()\n",
    "\n",
    "    hmdb51_test = AHMDB51(\n",
    "        num_workers=1,\n",
    "        frame_rate=5,\n",
    "        frames_per_clip=num_frames,\n",
    "        step_between_clips=clip_steps,\n",
    "        train=False,\n",
    "        transform=transform_test\n",
    "        )\n",
    "\n",
    "\n",
    "    return None, hmdb51_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd3014-1831-4312-b135-05a59b02f5f3",
   "metadata": {},
   "source": [
    "### Utility function to show a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6861e546-7dc7-408c-b13d-d42c9a0b6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import torchvision\n",
    "\n",
    "def show_tensor(tensor):\n",
    "    with open(\"tmp_video.mp4\", \"wb\") as f:\n",
    "        torchvision.io.write_video(f.name, tensor, fps=5, video_codec=\"h264\")\n",
    "        f.seek(0)\n",
    "        display(Video(\"tmp_video.mp4\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee562d4-3d97-44d6-8c47-191005ae60b4",
   "metadata": {},
   "source": [
    "## Instantiate a off the shelf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01a490-4cf8-4328-8112-ed172650300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movinets import MoViNet\n",
    "from movinets.config import _C\n",
    "\n",
    "# Use the original movinet based on Kinetics400 dataset when we get pretrained.\n",
    "model_vanilla = MoViNet(_C.MODEL.MoViNetA0, causal = False, pretrained = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7045eb-228a-4019-93a1-7fe69ee1068a",
   "metadata": {},
   "source": [
    "## Make a model from trained movinet with hmdb51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e4da4-e7b8-42e0-a5db-a2dfe6a5169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model trained on HMDB51. It has been trained for 1 epoch.\n",
    "model_trained = torch.load(\"movinet_hmdb51_1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0acc9c-6d9f-4330-82ed-b3affebd4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_data_sets()\n",
    "test.classes = {v: k for k, v in test.ci.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4bda9",
   "metadata": {},
   "source": [
    "## See the shape of the tensor passsed through model.\n",
    "\n",
    "This point is good to have, and troubleshoot any problems with the input going into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45065c28-1aa8-458c-9c87-b9435fd7f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test[333]\n",
    "video, audio, class_index = data\n",
    "print(video.shape)\n",
    "x = video.permute(1, 2, 3, 0)\n",
    "x=(x*255).type(torch.uint8)\n",
    "show_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919086f-3058-4887-b559-12ee21285e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = class_index\n",
    "all_classes = test.classes\n",
    "print(f\"{len(all_classes)=} \\r\\n {all_classes=}\\r\\n {all_classes[ground_truth]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb120c-32db-4210-9914-af0cd2f3dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra dim to video tensor to make it compatible with model.\n",
    "p = video[None, :]\n",
    "y = model_trained(p)\n",
    "\n",
    "# Get predictions from the trained movinet\n",
    "preds = torch.topk(y, 5, largest=True)\n",
    "\n",
    "#show the top k predictions.\n",
    "for i in preds.indices[0]:\n",
    "    print(test.classes[int(i)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f4ac2",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22c594-fa99-4981-9070-3aa658a18319",
   "metadata": {},
   "source": [
    "### Predict with vanilla\n",
    "\n",
    "Take a random clip from the test Dataset (specified as an index between 0 and len test)\n",
    "\n",
    "Some indices will be out of 51 range, as the model had 600 classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec98a8-0a26-4341-8406-094af799317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video, _, ground_truth=test[10]\n",
    "show_tensor((video.permute(1, 2, 3, 0)*255).type(torch.uint8))\n",
    "\n",
    "display(f\"{test.classes[ground_truth]=}\")\n",
    "\n",
    "y = model_vanilla(video[None, :])\n",
    "op = torch.nn.Softmax(dim=1)\n",
    "preds = torch.topk(op(y), 5, largest=True)\n",
    "print(\"Predictions:\")\n",
    "for i, prob in zip(preds.indices[0], preds.values[0]):\n",
    "    try:\n",
    "        prediction = test.classes[int(i)]\n",
    "        probability = float(prob)\n",
    "        print(f\"{prediction=}, {probability=}\")\n",
    "    except IndexError:\n",
    "        print(f\"Cannot find class for index={i}\")\n",
    "    except KeyError:\n",
    "        print(f\"Cannot find class for index={i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c3c9a-8f28-4188-b601-9f9f573c5ec9",
   "metadata": {},
   "source": [
    "### Predict with trained.\n",
    "\n",
    "Way better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30fea4-96ec-4354-a0d5-544ae592a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video, _, ground_truth=test[120]\n",
    "show_tensor((video.permute(1, 2, 3, 0)*255).type(torch.uint8))\n",
    "\n",
    "\n",
    "y = model_trained(video[None, :])\n",
    "op = torch.nn.Softmax(dim=1)\n",
    "preds = torch.topk(op(y), 5, largest=True)\n",
    "print(\"Predictions:\")\n",
    "for i, prob in zip(preds.indices[0], preds.values[0]):\n",
    "    try:\n",
    "        prediction = test.classes[int(i)]\n",
    "        probability = float(prob)\n",
    "        print(f\"{prediction=}, {probability=}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Cannot find class for index={i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
