{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55683ea8",
   "metadata": {},
   "source": [
    "# Building a RAG chain from a Website\n",
    "\n",
    "This notebook shows how to use ApertureDB as part of a Retrieval-Augmented Generation [Langchain](/Integrations/langchain_howto) pipeline.  This means that we're going to use ApertureDB as a vector-based search engine to find documents that match the query and then use a large-language model to generate an answer based on those documents. \n",
    "\n",
    "If you have already completed the notebook [Ingesting a Website into ApertureDB](./website_ingest), then your ApertureDB instance should already contain text from your chosen website.\n",
    "We'll use that to answer natural-language questions.\n",
    "\n",
    "![RAG workflow](images/RAG_Demo.png)\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8188f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet aperturedb langchain langchain-core langchain-community langchainhub gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd257925-73ab-4d76-a20a-708a255285d2",
   "metadata": {},
   "source": [
    "## Choose a prompt\n",
    "\n",
    "The prompt ties together the source documents and the user's query, and also sets some basic parameters for the chat engine.  You will get better results if you explain a little about the context for your chosen website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f3be8f-04ec-4376-886b-f15c0dca3cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following documents to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question}\n",
      "{context}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following documents to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "{context}\n",
    "Answer:\"\"\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e93b-aefd-4f6f-9b70-e3bbab075426",
   "metadata": {},
   "source": [
    "For comparison, we're also going to ask the same questions of the language model without using documents.  This prompt is for a non-RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919d7974-dd0f-4069-8a2b-9ee1b412f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Answer the question from your general knowledge.  If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt2 = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Answer the question from your general knowledge.  If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "print(prompt2.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863d40",
   "metadata": {},
   "source": [
    "## Choose an Embedding\n",
    "\n",
    "We have to use the same embedding that we used when we loaded the documents.\n",
    "Here we're using the GPT2All package and loading one of its smaller models.  Don't worry if you see messages about CUDA libraries being unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552cac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings dimension: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embeddings = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\")\n",
    "embeddings_dim = len(embeddings.embed_query(\"test\"))\n",
    "print(f\"Embeddings dimension: {embeddings_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaaef02",
   "metadata": {},
   "source": [
    "## Connect to ApertureDB\n",
    "\n",
    "For the next part, we need access to a specific ApertureDB instance.\n",
    "There are several ways to set this up.\n",
    "The code provided here will accept ApertureDB connection information as a JSON string.\n",
    "See our [Configuration](https://docs.aperturedata.io/Setup/client/configuration) help page for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! adb config create  --from-json --active "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282933d",
   "metadata": {},
   "source": [
    "## Create vectorstore\n",
    "\n",
    "Now we create a LangChain vectorstore object, backed by the ApertureDB instance we have already uploaded documents to.\n",
    "Remember to change the name of the DESCRIPTOR_SET if you changed it when you loaded the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ff395e-8e09-4e24-b96d-af3a9f3007f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ApertureDB\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "DESCRIPTOR_SET = \"test\"\n",
    "\n",
    "vectorstore = ApertureDB(embeddings=embeddings,\n",
    "                 descriptor_set=DESCRIPTOR_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c9537-eeff-411d-9a85-68b2e186d730",
   "metadata": {},
   "source": [
    "## Create a retriever\n",
    "\n",
    "The retriever is responsible for finding the most relevant documents in the vectorstore for a given query.  Here's we using the \"max marginal relevance\" retriever, which is a simple but effective way to find a diverse set of documents that are relevant to a query.  For each query, we retrieve the top 10 documents, but we do so by fetching 20 and then selecting the top 5 using the MMR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7e4bf2-c22c-496f-8b70-8adafc0ad649",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_type = \"mmr\" # \"similarity\" or \"mmr\"\n",
    "k = 4              # number of results used by LLM\n",
    "fetch_k = 20       # number of results fetched for MMR\n",
    "retriever = vectorstore.as_retriever(search_type=search_type,\n",
    "    search_kwargs=dict(k=k, fetch_k=fetch_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33d881-d5d1-442d-af01-70330ac6b736",
   "metadata": {},
   "source": [
    "## Select an LLM engine\n",
    "\n",
    "Here we're again using GPT4, but there's no need to use the same provider as we used for embeddings.  The model is around 4GB, so downloading it will take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1da3f60-d278-4e95-8ced-59a4fa2c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", allow_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815917b3-339c-447c-b1e9-a692f8462ae3",
   "metadata": {},
   "source": [
    "## Build the chain\n",
    "\n",
    "Now we put it all together.  The chain is responsible for taking a user query and returning a response.  It does this by first retrieving the most relevant documents using vector search, then using the LLM to generate a response.\n",
    "\n",
    "For demonstration purposes, we're printing the documents that were retrieved, but in a real application you would probably want to hide this information from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ad132e-8804-43d5-8da4-f852dd42d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Document {i}: \" + doc.page_content for i, doc in enumerate(docs, start=1))\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc6ccf-ab84-4213-b931-71a757321e91",
   "metadata": {},
   "source": [
    "This chain does not use RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dfd155b-4a04-4db7-b08c-f74a2d82c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_chain = (\n",
    "  {\"question\": RunnablePassthrough()}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bbb54-b743-4d24-b506-57c125ab2bc0",
   "metadata": {},
   "source": [
    "## Run the chain\n",
    "\n",
    "Now we can enter a query and see the response.\n",
    "We're using a local LLM and we may not have GPU, so this is likely to be slow.\n",
    "\n",
    "If you chose to crawl the ApertureDB documentation. here are some suggested questions:\n",
    "* How do I upload many descriptors to ApertureData?\n",
    "* How can I store audio files?\n",
    "* What support is there for PyTorch?\n",
    "* How can I use TensorBoard with ApertureDB?\n",
    "* How can I get an individual frame from a video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0cf16d-3c62-40ce-86ec-c12692b646be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### User Query\n",
       "What type of data will ApertureDB store?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Non-RAG Answer\n",
       " ApertureDB is a database designed to store genomic data such as DNA sequences, gene expression levels, and other types of biological information. It provides efficient querying capabilities for large-scale genomics studies. The stored data can be used for various bioinformatics applications like comparative genomics, evolutionary analysis, and disease research."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RAG Answer\n",
       " According to the provided documents, ApertureDB will store vector data. Specifically, it is both a vector store and a graph database, with current support for storing and retrieving vectors through LangChain's API. In the future, it plans to add support for its graph database functionality as well. (3 sentences) 1/2\n",
       "Final Answer: The final answer is that ApertureDB will store vector data; specifically, it is both a vector store and a graph database with current support for storing and retrieving vectors through LangChain's API in the future, it plans to add support for its graph database functionality as well. I hope it is correct. 2/2\n",
       "Final Answer: The final answer is that ApertureDB will store vector data; specifically, it is both a vector store and a graph database with current support for storing and retrieving vectors through LangChain's API in the future, it plans to add support for its graph database functionality as well. I hope it is correct. 2/2\n",
       "Final Answer: The final answer is that ApertureDB will store vector data; specifically, it is both a vector store and a graph database with current support for storing and retrieving vectors through LangChain's API in the future, it plans to add support for\n",
       "### Documents\n",
       "1. **[Introduction and Usage Examples | ApertureDB](https://docs.aperturedata.io/python_sdk/cli/usage)**: available to\n",
       "load data into an instance of ApertureDB.\n",
       "Let's assume your data has been saved into a \n",
       "CSV file\n",
       "2. **[LangChain | ApertureDB](https://docs.aperturedata.io/Integrations/langchain_howto)**: retriever\n",
       ".  This is useful for using an ApertureDB vector store as part of pipelines such as RAG.  \n",
       "Graph database\n",
       "​\n",
       "It is possible to use a lot of ApertureDB's functionality through LangChain, but the full power of ApertureDB is only available through the ApertureDB API.  For example, you can use LangChain to store and retrieve vectors from ApertureDB, and then use the ApertureDB API to query the graph database.\n",
       "3. **[LangChain | ApertureDB](https://docs.aperturedata.io/Integrations/langchain_howto)**: ApertureDB is both a vector store and a graph database.  Currently, LangChain supports the vector store functionality of ApertureDB.  This means that you can use ApertureDB as a provider for LangChain's vector store.  This allows you to store and retrieve vectors from ApertureDB using LangChain's API.\n",
       "In the future, we plan to add support for ApertureDB's graph database functionality to LangChain.  This will allow you to store and query graphs in ApertureDB using LangChain's API.\n",
       "Vector Store\n",
       "​\n",
       "4. **[LangChain | ApertureDB](https://docs.aperturedata.io/Integrations/langchain_howto)**: In the future, we plan to add support for ApertureDB's graph database functionality to LangChain.  This will allow you to store and query graphs in ApertureDB using LangChain's API.  This will make it easier to use ApertureDB as a graph database in LangChain applications.\n",
       "Implementation details\n",
       "​\n",
       "Those attempting a hybrid approach should note a few of details of how LangChain vectore stores and documents are represented internally in ApertureDB:\n",
       "The LangChain vector store corresponds to a \n",
       "DescriptorSet"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def run_query(user_query):\n",
    "    display(Markdown(f\"### User Query\\n{user_query}\"))\n",
    "\n",
    "    nonrag_answer = plain_chain.invoke(user_query)\n",
    "    display(Markdown(f\"### Non-RAG Answer\\n{nonrag_answer}\"))\n",
    "\n",
    "    rag_answer = rag_chain_with_source.invoke(user_query)\n",
    "\n",
    "    display(Markdown(\"\\n\".join([\n",
    "        f\"### RAG Answer\\n{rag_answer['answer']}\",\n",
    "        f\"### Documents\",\n",
    "        *(f\"{i}. **[{doc.metadata['title']}]({doc.metadata['url']})**: {doc.page_content}\" for i, doc in enumerate(rag_answer[\"context\"], 1))\n",
    "    ])))\n",
    "\n",
    "\n",
    "user_query = input(\"Enter a question:\")\n",
    "assert user_query, \"Please enter a question.\"\n",
    "run_query(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
