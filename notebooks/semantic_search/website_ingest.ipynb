{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting a Website into ApertureDB\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to prepare for running a RAG chain. \n",
    "\n",
    "We will:\n",
    "* Crawl a website\n",
    "* Scrape its contents\n",
    "* Split it into parts\n",
    "* Generate embeddings\n",
    "* Load the documents into ApertureDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installations\n",
    "\n",
    "\n",
    "### Data management with ApertureDB\n",
    "ApertureDB is set up as a database (server) and can be accessed from clients anywhere as long as the server is accessible on the network to the client.\n",
    "\n",
    "Sign up for an Aperture [cloud account here](https://cloud.aperturedata.io) (30 days free trial) or see [other methods here](http://docs.aperturedata.io/category/setup-server)\n",
    "\n",
    "### Connection method with ApertureDB\n",
    "\n",
    "Installation instructions for the various packages needed for this application are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade aperturedb langchain langchain-community langchainhub scrapy gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect ApertureDB Client and Server\n",
    "\n",
    "Detailed instructions for configuring your client can be found [on this page](https://docs.aperturedata.io/Setup/client/configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!adb config create --overwrite --active --from-json rag_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration that you have connected to the server successfully, let's see a summary of the database schema.  \n",
    "The first time you do this, you may need to grant permission for this notebook to access your secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Summary ==================\n",
      "Database: ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io\n",
      "Version: 0.18.3\n",
      "Status:  0\n",
      "Info:    OK\n",
      "------------------ Entities -----------------\n",
      "Total entities types:    2\n",
      "_Descriptor         \n",
      "  Total elements: 1420\n",
      "    String   | _label    |      1420 (100%)\n",
      "    String   | lc_title  |      1420 (100%)\n",
      "    String   | lc_url    |      1420 (100%)\n",
      "    String   | text      |      1420 (100%)\n",
      "I   String   | uniqueid  |      1420 (100%)\n",
      "_DescriptorSet      \n",
      "  Total elements: 1\n",
      "    Number   | _dimensions  |         1 (100%)\n",
      "I   String   | _name        |         1 (100%)\n",
      "---------------- Connections ----------------\n",
      "Total connections types: 1\n",
      "_DescriptorSetToDescriptor\n",
      "  _DescriptorSet ====> _Descriptor\n",
      "  Total elements: 1420\n",
      "------------------ Totals -------------------\n",
      "Total nodes: 1421\n",
      "Total edges: 1420\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from aperturedb.Utils import Utils\n",
    "from aperturedb.CommonLibrary import create_connector\n",
    "\n",
    "# Create the connector for ApertureDB\n",
    "client = create_connector()\n",
    "\n",
    "# Use the connector to create a Utils object and print the summary\n",
    "utils = Utils(client)\n",
    "utils.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We need to import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.crawler import Crawler, CrawlerProcess\n",
    "from scrapy.http import HtmlResponse\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import ApertureDB\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from urllib.parse import urlparse\n",
    "import argparse\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the Website\n",
    "\n",
    "We're going to use the `scrapy` package to crawl a website for us.\n",
    "We're going to add a small wrapper around it to make it play well with LangChain.\n",
    "\n",
    "### LangChainSpider\n",
    "\n",
    "First we create a wrapper for `scrapy`'s `CrawlSpider` that generates LangChain `Document`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainSpider(CrawlSpider):\n",
    "    name = \"langchain_spider\"\n",
    "    rules = [Rule(LinkExtractor(), callback='parse', follow=True)]\n",
    "    start_urls = [\"https://docs.aperturedata.io/\"]\n",
    "    _follow_links = True\n",
    "\n",
    "    def __init__(self, start_url, css_selector=None, **kwargs):\n",
    "        \"\"\"LangChain Spider\n",
    "\n",
    "        Args:\n",
    "            start_url (str): The URL to start crawling from\n",
    "            css_selector (str, optional): The CSS selector to use to extract text from the page. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_urls = (start_url,)\n",
    "        # Extract the domain from the URL; we only want to crawl the same domain\n",
    "        self.allowed_domains = list(\n",
    "            set([urlparse(url).netloc for url in self.start_urls]))\n",
    "        self.css_selector = css_selector\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, **kwargs):\n",
    "        \"\"\"Factory method to create a new instance of the spider\n",
    "\n",
    "        Gets arguments from crawler settings.\n",
    "\n",
    "        Args:\n",
    "            crawler (Crawler): The Scrapy Crawler instance\n",
    "\n",
    "        Returns:\n",
    "            LangChainSpider: A new instance of the spider\n",
    "        \"\"\"\n",
    "        settings = crawler.settings\n",
    "        args = settings.get(\"LANGCHAIN_PIPELINE_ARGS\", {})\n",
    "        spider = cls(start_url=args.start_url,\n",
    "                     css_selector=args.selector, crawler=crawler, **kwargs)\n",
    "        return spider\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse the response from the page and yield a Document\n",
    "\n",
    "        Args:\n",
    "            response: The parsed response from the page\n",
    "\n",
    "        Yields:\n",
    "            Document: A LangChain document object containing the page content\n",
    "        \"\"\"\n",
    "        if isinstance(response, HtmlResponse):  # Ignore anything that is not HTML\n",
    "            if self.css_selector:\n",
    "                elements = response.css(self.css_selector).xpath(\".//text()\").getall()\n",
    "            else:\n",
    "                elements = response.xpath('//body//text()').getall()\n",
    "\n",
    "            content = \"\\n\".join(elements).strip()\n",
    "            title = response.css(\"title::text\").get() # extract the title of the page\n",
    "            logging.info(f\"URL: {response.url}, Title: {title} Content: {len(content)}\")\n",
    "            if content:\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    id=response.url, # Use the URL as the document ID\n",
    "                    metadata={\n",
    "                        \"url\": response.url,\n",
    "                        \"title\": title,\n",
    "                    }\n",
    "                )\n",
    "                yield doc\n",
    "            else:\n",
    "                logging.warning(f\"Empty content for URL: {response.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChainPipeline\n",
    "\n",
    "Now we create a pipeline that's going to be called by the crawler to process those documents.\n",
    "This is the part where we call ApertureDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPipeline:\n",
    "    def __init__(self, vectorstore, splitter=None):\n",
    "        \"\"\"Crawler pipeline for taking LangChain documents and adding them to a vector store\n",
    "\n",
    "        Args:\n",
    "            vectorstore (VectorStore): The vector store to add the documents to\n",
    "            splitter (function, optional): A function to split the documents into smaller chunks. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.vectorstore = vectorstore\n",
    "        self.splitter = splitter\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        \"\"\"Factory method to create a new instance of the pipeline\n",
    "\n",
    "        Gets arguments from crawler settings.\n",
    "\n",
    "        Args:\n",
    "            crawler (Crawler): The Scrapy Crawler instance\n",
    "\n",
    "        Returns:\n",
    "            LangChainPipeline: A new instance of the pipeline\n",
    "        \"\"\"\n",
    "        settings = crawler.settings\n",
    "        args = settings.get(\"LANGCHAIN_PIPELINE_ARGS\", {})\n",
    "\n",
    "        # The embeddings are a GPT4ALL model\n",
    "        embeddings = GPT4AllEmbeddings(model_name=args.embeddings)\n",
    "        embeddings_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "        # The vector store is an ApertureDB instance\n",
    "        vectorstore = ApertureDB(descriptor_set=args.descriptorset,\n",
    "                                 embeddings=embeddings,\n",
    "                                 dimensions=embeddings_dim)\n",
    "\n",
    "        # The splitter is a RecursiveCharacterTextSplitter, configured from arguments\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap).split_documents\n",
    "\n",
    "        return cls(vectorstore=vectorstore, splitter=splitter)\n",
    "\n",
    "    def process_item(self, doc, spider):\n",
    "        \"\"\"Process the document and add it to the vector store\n",
    "        \n",
    "        Args:\n",
    "            doc (Document): The LangChain document object\n",
    "            spider (LangChainSpider): The spider that parsed the document\n",
    "        \"\"\"\n",
    "        docs = [doc]\n",
    "        if self.splitter:\n",
    "            docs = self.splitter(docs)\n",
    "            logging.info(f\"Splitting document into {len(docs)} chunks\")\n",
    "        self.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure our crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = \"INFO\"\n",
    "max_pages = 1000\n",
    "concurrent_requests_per_domain = 32\n",
    "concurrent_requests = 64\n",
    "class Args:\n",
    "    start_url = \"https://docs.aperturedata.io/\"\n",
    "    descriptorset = \"test\"\n",
    "    chunk_size = 512\n",
    "    chunk_overlap = 64\n",
    "    embeddings = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "    selector = \".markdown\"\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the crawl\n",
    "\n",
    "Create the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 17:52:41 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2024-12-20 17:52:41 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0], pyOpenSSL 24.3.0 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-5.15.0-122-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "crawler = CrawlerProcess(\n",
    "    settings={\n",
    "        \"LOG_LEVEL\": log_level,\n",
    "        \"ITEM_PIPELINES\": {\n",
    "            LangChainPipeline: 1000,\n",
    "        },\n",
    "        \"LANGCHAIN_PIPELINE_ARGS\": args,\n",
    "        # Limit number of pages processed (not crawled)\n",
    "        \"CLOSESPIDER_ITEMCOUNT\": max_pages,\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': concurrent_requests_per_domain,\n",
    "        'CONCURRENT_REQUESTS': concurrent_requests,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the crawl.  This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the vector store before starting\n",
    "ApertureDB.delete_vectorstore(args.descriptorset)\n",
    "\n",
    "crawler.crawl(LangChainSpider)\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can list the vectorstores in our ApertureDB instance.\n",
    "Notice the `count` field for the `test` vectorstore.\n",
    "Remember that this is the number of segments, which will be more than the number of pages crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 17:57:47 [aperturedb.CommonLibrary] WARNING: Utils.create_connector is deprecated and will be removed in a future release. Use CommonLibrary.create_connector instead.\n",
      "2024-12-20 17:57:47 [aperturedb.CommonLibrary] INFO: Using active configuration 'rag_demo'\n",
      "2024-12-20 17:57:47 [aperturedb.CommonLibrary] INFO: Configuration: [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_count': 1850,\n",
       "  '_dimensions': 384,\n",
       "  '_engines': ['HNSW'],\n",
       "  '_metrics': ['CS'],\n",
       "  '_name': 'test',\n",
       "  '_uniqueid': '2.0.17280'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ApertureDB.list_vectorstores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Next you want to use this vectorstore to drive a RAG (Retrieval-Augmented Generation) chain.\n",
    "\n",
    "See [Building a RAG Chain from a Website](https://docs.aperturedata.io/HowToGuides/Applications/website_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further information\n",
    "\n",
    "* [LangChain vectorstore integration](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.aperturedb.ApertureDB.html)\n",
    "* [ApertureDB documentation website](https://docs.aperturedata.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
