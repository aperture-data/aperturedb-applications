{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting a Website into ApertureDB\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonmstrate how to prepare for running a RAG chain. \n",
    "\n",
    "We will:\n",
    "* Crawl a website\n",
    "* Scrape its contents\n",
    "* Split it into parts\n",
    "* Generate embeddings\n",
    "* Load the documents into ApertureDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installations\n",
    "\n",
    "\n",
    "### Data management with ApertureDB\n",
    "ApertureDB is set up as a database (server) and can be accessed from clients anywhere as long as the server is accessible on the network to the client.\n",
    "\n",
    "Sign up for an Aperture [cloud account here](https://cloud.aperturedata.io) (30 days free trial) or see [other methods here](http://docs.aperturedata.io/category/setup-server)\n",
    "\n",
    "### Connection method with ApertureDB\n",
    "\n",
    "Installation instructions for the various packages needed for this application are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade aperturedb langchain langchain-community langchainhub scrapy gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect ApertureDB Client and Server\n",
    "\n",
    "Detailed instructions for configuring your client can be found [on this page](https://docs.aperturedata.io/Setup/client/configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!adb config create --overwrite --active --from-json rag_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration that you have connected to the server successfully, let's see a summary of the database schema.  \n",
    "The first time you do this, you may need to grant permission for this notebook to access your secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Summary ==================\n",
      "Database: ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io\n",
      "Version: 0.18.3\n",
      "Status:  0\n",
      "Info:    OK\n",
      "------------------ Entities -----------------\n",
      "Total entities types:    2\n",
      "_Descriptor         \n",
      "  Total elements: 249\n",
      "    String   | _label    |       249 (100%)\n",
      "    String   | lc_title  |       249 (100%)\n",
      "    String   | lc_url    |       249 (100%)\n",
      "    String   | text      |       249 (100%)\n",
      "I   String   | uniqueid  |       249 (100%)\n",
      "_DescriptorSet      \n",
      "  Total elements: 1\n",
      "    Number   | _dimensions  |         1 (100%)\n",
      "I   String   | _name        |         1 (100%)\n",
      "---------------- Connections ----------------\n",
      "Total connections types: 1\n",
      "_DescriptorSetToDescriptor\n",
      "  _DescriptorSet ====> _Descriptor\n",
      "  Total elements: 249\n",
      "------------------ Totals -------------------\n",
      "Total nodes: 250\n",
      "Total edges: 249\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from aperturedb.Utils import Utils\n",
    "from aperturedb.CommonLibrary import create_connector\n",
    "\n",
    "# Create the connector for ApertureDB\n",
    "client = create_connector()\n",
    "\n",
    "# Use the connector to create a Utils object and print the summary\n",
    "utils = Utils(client)\n",
    "utils.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We need to import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.crawler import Crawler, CrawlerProcess\n",
    "from scrapy.http import HtmlResponse\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import ApertureDB\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from urllib.parse import urlparse\n",
    "import argparse\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the Website\n",
    "\n",
    "We're going to use the `scrapy` package to crawl a website for us.\n",
    "We're going to add a small wrapper around it to make it play well with LangChain.\n",
    "\n",
    "### LangChainSpider\n",
    "\n",
    "First we create a wrapper for `scrapy`'s `CrawlSpider` that generates LangChain `Document`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainSpider(CrawlSpider):\n",
    "    name = \"langchain_spider\"\n",
    "    rules = [Rule(LinkExtractor(), callback='parse', follow=True)]\n",
    "    start_urls = [\"https://docs.aperturedata.io/\"]\n",
    "    _follow_links = True\n",
    "\n",
    "    def __init__(self, start_url, css_selector=None, **kwargs):\n",
    "        \"\"\"LangChain Spider\n",
    "\n",
    "        Args:\n",
    "            start_url (str): The URL to start crawling from\n",
    "            css_selector (str, optional): The CSS selector to use to extract text from the page. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_urls = (start_url,)\n",
    "        # Extract the domain from the URL; we only want to crawl the same domain\n",
    "        self.allowed_domains = list(\n",
    "            set([urlparse(url).netloc for url in self.start_urls]))\n",
    "        self.css_selector = css_selector\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, **kwargs):\n",
    "        \"\"\"Factory method to create a new instance of the spider\n",
    "\n",
    "        Gets arguments from crawler settings.\n",
    "\n",
    "        Args:\n",
    "            crawler (Crawler): The Scrapy Crawler instance\n",
    "\n",
    "        Returns:\n",
    "            LangChainSpider: A new instance of the spider\n",
    "        \"\"\"\n",
    "        settings = crawler.settings\n",
    "        args = settings.get(\"LANGCHAIN_PIPELINE_ARGS\", {})\n",
    "        spider = cls(start_url=args.start_url,\n",
    "                     css_selector=args.selector, crawler=crawler, **kwargs)\n",
    "        return spider\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse the response from the page and yield a Document\n",
    "\n",
    "        Args:\n",
    "            response: The parsed response from the page\n",
    "\n",
    "        Yields:\n",
    "            Document: A LangChain document object containing the page content\n",
    "        \"\"\"\n",
    "        if isinstance(response, HtmlResponse):  # Ignore anything that is not HTML\n",
    "            if self.css_selector:\n",
    "                elements = response.css(self.css_selector).xpath(\".//text()\").getall()\n",
    "            else:\n",
    "                elements = response.xpath('//body//text()').getall()\n",
    "\n",
    "            content = \"\\n\".join(elements).strip()\n",
    "            title = response.css(\"title::text\").get() # extract the title of the page\n",
    "            logging.info(f\"URL: {response.url}, Title: {title} Content: {len(content)}\")\n",
    "            if content:\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    id=response.url, # Use the URL as the document ID\n",
    "                    metadata={\n",
    "                        \"url\": response.url,\n",
    "                        \"title\": title,\n",
    "                    }\n",
    "                )\n",
    "                yield doc\n",
    "            else:\n",
    "                logging.warning(f\"Empty content for URL: {response.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChainPipeline\n",
    "\n",
    "Now we create a pipeline that's going to be called by the crawler to process those documents.\n",
    "This is the part where we call ApertureDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPipeline:\n",
    "    def __init__(self, vectorstore, splitter=None):\n",
    "        \"\"\"Crawler pipeline for taking LangChain documents and adding them to a vector store\n",
    "\n",
    "        Args:\n",
    "            vectorstore (VectorStore): The vector store to add the documents to\n",
    "            splitter (function, optional): A function to split the documents into smaller chunks. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.vectorstore = vectorstore\n",
    "        self.splitter = splitter\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        \"\"\"Factory method to create a new instance of the pipeline\n",
    "\n",
    "        Gets arguments from crawler settings.\n",
    "\n",
    "        Args:\n",
    "            crawler (Crawler): The Scrapy Crawler instance\n",
    "\n",
    "        Returns:\n",
    "            LangChainPipeline: A new instance of the pipeline\n",
    "        \"\"\"\n",
    "        settings = crawler.settings\n",
    "        args = settings.get(\"LANGCHAIN_PIPELINE_ARGS\", {})\n",
    "\n",
    "        # The embeddings are a GPT4ALL model\n",
    "        embeddings = GPT4AllEmbeddings(model_name=args.embeddings)\n",
    "        embeddings_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "        # The vector store is an ApertureDB instance\n",
    "        vectorstore = ApertureDB(descriptor_set=args.descriptorset,\n",
    "                                 embeddings=embeddings,\n",
    "                                 dimensions=embeddings_dim)\n",
    "\n",
    "        # The splitter is a RecursiveCharacterTextSplitter, configured from arguments\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap).split_documents\n",
    "\n",
    "        return cls(vectorstore=vectorstore, splitter=splitter)\n",
    "\n",
    "    def process_item(self, doc, spider):\n",
    "        \"\"\"Process the document and add it to the vector store\n",
    "        \n",
    "        Args:\n",
    "            doc (Document): The LangChain document object\n",
    "            spider (LangChainSpider): The spider that parsed the document\n",
    "        \"\"\"\n",
    "        docs = [doc]\n",
    "        if self.splitter:\n",
    "            docs = self.splitter(docs)\n",
    "            logging.info(f\"Splitting document into {len(docs)} chunks\")\n",
    "        self.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure our crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = \"INFO\"\n",
    "max_pages = 10\n",
    "class Args:\n",
    "    start_url = \"https://docs.aperturedata.io/\"\n",
    "    descriptorset = \"test\"\n",
    "    chunk_size = 512\n",
    "    chunk_overlap = 64\n",
    "    embeddings = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "    selector = \".markdown\"\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 05:24:20 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2024-12-20 05:24:20 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0], pyOpenSSL 24.3.0 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-5.15.0-122-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "crawler = CrawlerProcess(\n",
    "    settings={\n",
    "        \"LOG_LEVEL\": log_level,\n",
    "        \"ITEM_PIPELINES\": {\n",
    "            LangChainPipeline: 100,\n",
    "        },\n",
    "        \"LANGCHAIN_PIPELINE_ARGS\": args,\n",
    "        # Limit number of pages processed (not crawled)\n",
    "        \"CLOSESPIDER_ITEMCOUNT\": max_pages,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 05:24:20 [aperturedb.CommonLibrary] WARNING: Utils.create_connector is deprecated and will be removed in a future release. Use CommonLibrary.create_connector instead.\n",
      "2024-12-20 05:24:20 [aperturedb.CommonLibrary] INFO: Using active configuration 'rag_demo'\n",
      "2024-12-20 05:24:20 [aperturedb.CommonLibrary] INFO: Configuration: [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:22 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-20 05:24:22 [scrapy.extensions.telnet] INFO: Telnet Password: 689d3c82f4df2e80\n",
      "2024-12-20 05:24:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.closespider.CloseSpider',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-20 05:24:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CLOSESPIDER_ITEMCOUNT': 10, 'LOG_LEVEL': 'INFO'}\n",
      "2024-12-20 05:24:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-20 05:24:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-20 05:24:23 [aperturedb.CommonLibrary] WARNING: Utils.create_connector is deprecated and will be removed in a future release. Use CommonLibrary.create_connector instead.\n",
      "2024-12-20 05:24:23 [aperturedb.CommonLibrary] INFO: Using active configuration 'rag_demo'\n",
      "2024-12-20 05:24:23 [aperturedb.CommonLibrary] INFO: Configuration: [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[<class '__main__.LangChainPipeline'>]\n",
      "2024-12-20 05:24:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-20 05:24:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-20 05:24:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6046\n",
      "2024-12-20 05:24:26 [root] INFO: URL: https://docs.aperturedata.io/Introduction/WhyAperture, Title: Why ApertureDB? | ApertureDB Content: 4903\n",
      "2024-12-20 05:24:26 [root] INFO: Splitting document into 12 chunks\n",
      "2024-12-20 05:24:26 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:26 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:26 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:26 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/get-started, Title: Get Started | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/get-started\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/database-concepts, Title: Database Concepts | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/database-concepts\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/aperturedb-query-language, Title: ApertureDB Query Language | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/aperturedb-query-language\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/how-to-guides, Title: How to Guides | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/how-to-guides\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/integrations, Title: Integrations | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/integrations\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/category/aperturedb-python-sdk, Title: ApertureDB Python SDK | ApertureDB Content: 0\n",
      "2024-12-20 05:24:28 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/aperturedb-python-sdk\n",
      "2024-12-20 05:24:28 [root] INFO: URL: https://docs.aperturedata.io/concepts/Terms, Title: ApertureDB Terminology | ApertureDB Content: 3004\n",
      "2024-12-20 05:24:28 [root] INFO: Splitting document into 7 chunks\n",
      "2024-12-20 05:24:28 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:28 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:28 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:28 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:29 [root] INFO: URL: https://docs.aperturedata.io/Introduction/Resources, Title: Resources | ApertureDB Content: 5171\n",
      "2024-12-20 05:24:29 [root] INFO: Splitting document into 14 chunks\n",
      "2024-12-20 05:24:29 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:29 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:29 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:29 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:30 [root] INFO: URL: https://docs.aperturedata.io/category/introduction, Title: Introduction | ApertureDB Content: 0\n",
      "2024-12-20 05:24:30 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/introduction\n",
      "2024-12-20 05:24:31 [root] INFO: URL: https://docs.aperturedata.io/category/production-deployment-and-administration, Title: Production Deployment and Administration | ApertureDB Content: 0\n",
      "2024-12-20 05:24:31 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/production-deployment-and-administration\n",
      "2024-12-20 05:24:31 [root] INFO: URL: https://docs.aperturedata.io/administration/troubleshooting, Title: Troubleshooting | ApertureDB Content: 1754\n",
      "2024-12-20 05:24:31 [root] INFO: Splitting document into 5 chunks\n",
      "2024-12-20 05:24:31 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:31 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:31 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:31 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:32 [root] INFO: URL: https://docs.aperturedata.io/category/reference, Title: Reference | ApertureDB Content: 0\n",
      "2024-12-20 05:24:32 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/reference\n",
      "2024-12-20 05:24:32 [root] INFO: URL: https://docs.aperturedata.io/administration/faq, Title: Frequently Asked Questions | ApertureDB Content: 12727\n",
      "2024-12-20 05:24:32 [root] INFO: Splitting document into 48 chunks\n",
      "2024-12-20 05:24:32 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:32 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:32 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:32 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:33 [root] INFO: URL: https://docs.aperturedata.io/category/command-line-tool-adb, Title: Command Line Tool (adb) | ApertureDB Content: 0\n",
      "2024-12-20 05:24:33 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/command-line-tool-adb\n",
      "2024-12-20 05:24:33 [root] INFO: URL: https://docs.aperturedata.io/, Title: ApertureDB's Documentation | ApertureDB Content: 1095\n",
      "2024-12-20 05:24:33 [root] INFO: Splitting document into 3 chunks\n",
      "2024-12-20 05:24:34 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:34 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:34 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:34 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:35 [root] INFO: URL: https://docs.aperturedata.io/Setup/QuickStart, Title: Quick Start | ApertureDB Content: 6606\n",
      "2024-12-20 05:24:35 [root] INFO: Splitting document into 16 chunks\n",
      "2024-12-20 05:24:35 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:35 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:35 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:35 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:36 [root] INFO: URL: https://docs.aperturedata.io/category/helper-modules, Title: Helper Modules | ApertureDB Content: 0\n",
      "2024-12-20 05:24:36 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/helper-modules\n",
      "2024-12-20 05:24:36 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Applications/semantic_search/website_search, Title: Building a RAG chain from a Website | ApertureDB Content: 11690\n",
      "2024-12-20 05:24:36 [root] INFO: Splitting document into 29 chunks\n",
      "2024-12-20 05:24:37 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:37 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:37 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:37 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:38 [root] INFO: URL: https://docs.aperturedata.io/category/query-languages, Title: Query Languages | ApertureDB Content: 0\n",
      "2024-12-20 05:24:38 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/query-languages\n",
      "2024-12-20 05:24:38 [root] INFO: URL: https://docs.aperturedata.io/Integrations/langchain_howto, Title: LangChain | ApertureDB Content: 2397\n",
      "2024-12-20 05:24:38 [root] INFO: Splitting document into 6 chunks\n",
      "2024-12-20 05:24:38 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:38 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:38 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:38 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:39 [root] INFO: URL: https://docs.aperturedata.io/category/parallel-execution, Title: Parallel Execution | ApertureDB Content: 0\n",
      "2024-12-20 05:24:39 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/parallel-execution\n",
      "2024-12-20 05:24:39 [root] INFO: URL: https://docs.aperturedata.io/Integrations/machine_learning_howto, Title: Training and Inference | ApertureDB Content: 4312\n",
      "2024-12-20 05:24:39 [root] INFO: Splitting document into 12 chunks\n",
      "2024-12-20 05:24:39 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:40 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:40 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:40 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:41 [scrapy.core.engine] INFO: Closing spider (closespider_itemcount)\n",
      "2024-12-20 05:24:41 [root] INFO: URL: https://docs.aperturedata.io/category/database-connection-classes, Title: Database Connection Classes | ApertureDB Content: 0\n",
      "2024-12-20 05:24:41 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/database-connection-classes\n",
      "2024-12-20 05:24:41 [root] INFO: URL: https://docs.aperturedata.io/category/object-mapper-classes, Title: Object Mapper Classes | ApertureDB Content: 0\n",
      "2024-12-20 05:24:41 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/object-mapper-classes\n",
      "2024-12-20 05:24:41 [root] INFO: URL: https://docs.aperturedata.io/category/dataset-reference, Title: Dataset Reference | ApertureDB Content: 0\n",
      "2024-12-20 05:24:41 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/dataset-reference\n",
      "2024-12-20 05:24:41 [root] INFO: URL: https://docs.aperturedata.io/category/data-loader-reference, Title: Data Loader Reference | ApertureDB Content: 0\n",
      "2024-12-20 05:24:41 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/data-loader-reference\n",
      "2024-12-20 05:24:41 [root] INFO: URL: https://docs.aperturedata.io/query_language/Reference/shared_command_parameters/video_position, Title: Parameters for Positional Media Access | ApertureDB Content: 9036\n",
      "2024-12-20 05:24:41 [root] INFO: Splitting document into 20 chunks\n",
      "2024-12-20 05:24:41 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:41 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:41 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:41 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:42 [root] INFO: URL: https://docs.aperturedata.io/category/shared-parameter-classes, Title: Shared Parameter Classes | ApertureDB Content: 0\n",
      "2024-12-20 05:24:42 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/shared-parameter-classes\n",
      "2024-12-20 05:24:42 [root] INFO: URL: https://docs.aperturedata.io/Setup/client/configuration, Title: Configuration | ApertureDB Content: 4295\n",
      "2024-12-20 05:24:42 [root] INFO: Splitting document into 10 chunks\n",
      "2024-12-20 05:24:43 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:43 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:43 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:43 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:44 [root] INFO: URL: https://docs.aperturedata.io/Integrations/labeling_pipeline_howto, Title: Labeling | ApertureDB Content: 7218\n",
      "2024-12-20 05:24:44 [root] INFO: Splitting document into 16 chunks\n",
      "2024-12-20 05:24:44 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:44 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:44 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:44 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:45 [root] INFO: URL: https://docs.aperturedata.io/category/build-applications, Title: Build Applications | ApertureDB Content: 0\n",
      "2024-12-20 05:24:45 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/build-applications\n",
      "2024-12-20 05:24:45 [root] INFO: URL: https://docs.aperturedata.io/category/build-ml-examples, Title: Build ML Examples | ApertureDB Content: 0\n",
      "2024-12-20 05:24:45 [root] WARNING: Empty content for URL: https://docs.aperturedata.io/category/build-ml-examples\n",
      "2024-12-20 05:24:45 [root] INFO: URL: https://docs.aperturedata.io/Setup/client/notebooks?notebooks=notebook, Title: Notebooks | ApertureDB Content: 2186\n",
      "2024-12-20 05:24:45 [root] INFO: Splitting document into 5 chunks\n",
      "2024-12-20 05:24:45 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:45 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:45 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:45 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:47 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Ingestion/Ingestion/Examples/Loading_CSVParsers, Title: Ingest Cookbook (CSVParser) | ApertureDB Content: 3820\n",
      "2024-12-20 05:24:47 [root] INFO: Splitting document into 10 chunks\n",
      "2024-12-20 05:24:47 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:47 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:47 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:47 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:48 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/cli/usage, Title: Introduction and Usage Examples | ApertureDB Content: 2842\n",
      "2024-12-20 05:24:48 [root] INFO: Splitting document into 8 chunks\n",
      "2024-12-20 05:24:48 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:48 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:48 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:48 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:49 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/cli/reference, Title: adb | ApertureDB Content: 7775\n",
      "2024-12-20 05:24:49 [root] INFO: Splitting document into 18 chunks\n",
      "2024-12-20 05:24:50 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:50 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:50 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:50 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:51 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/parallel_exec/DaskManager, Title: DaskManager | ApertureDB Content: 149\n",
      "2024-12-20 05:24:51 [root] INFO: Splitting document into 1 chunks\n",
      "2024-12-20 05:24:51 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:51 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:51 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:51 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:52 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/helpers/Subscriptable, Title: Subscriptable | ApertureDB Content: 323\n",
      "2024-12-20 05:24:52 [root] INFO: Splitting document into 1 chunks\n",
      "2024-12-20 05:24:52 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:52 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:52 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:52 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:53 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/helpers/NotebookHelpers, Title: NotebookHelpers | ApertureDB Content: 1591\n",
      "2024-12-20 05:24:53 [root] INFO: Splitting document into 4 chunks\n",
      "2024-12-20 05:24:54 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:54 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:54 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:54 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:55 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/helpers/ProgressBar, Title: ProgressBar | ApertureDB Content: 11\n",
      "2024-12-20 05:24:55 [root] INFO: Splitting document into 1 chunks\n",
      "2024-12-20 05:24:55 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:55 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:55 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:55 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:56 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/helpers/QueryGenerator, Title: QueryGenerator | ApertureDB Content: 140\n",
      "2024-12-20 05:24:56 [root] INFO: Splitting document into 1 chunks\n",
      "2024-12-20 05:24:56 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:56 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:56 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:56 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:57 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/helpers/Stats, Title: Stats | ApertureDB Content: 5\n",
      "2024-12-20 05:24:57 [root] INFO: Splitting document into 1 chunks\n",
      "2024-12-20 05:24:57 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:57 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:57 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:57 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:24:59 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Advanced/loaders, Title: Interact with PyTorch Objects | ApertureDB Content: 9277\n",
      "2024-12-20 05:24:59 [root] INFO: Splitting document into 22 chunks\n",
      "2024-12-20 05:24:59 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:24:59 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:24:59 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:24:59 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:00 [root] INFO: URL: https://docs.aperturedata.io/python_sdk/datasets/KaggleData, Title: KaggleData | ApertureDB Content: 1413\n",
      "2024-12-20 05:25:00 [root] INFO: Splitting document into 3 chunks\n",
      "2024-12-20 05:25:00 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:25:00 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:25:00 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:25:00 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:02 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Ingestion/Ingestion/PopularFormats, Title: Online Dataset Formats | ApertureDB Content: 3552\n",
      "2024-12-20 05:25:02 [root] INFO: Splitting document into 9 chunks\n",
      "2024-12-20 05:25:02 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:25:02 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:25:02 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:25:02 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:03 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Applications/semantic_search/website_ingest, Title: Ingesting a Website into ApertureDB | ApertureDB Content: 4984\n",
      "2024-12-20 05:25:03 [root] INFO: Splitting document into 13 chunks\n",
      "2024-12-20 05:25:03 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:25:03 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:25:03 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:25:03 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:04 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Applications/semantic_search/cohere_wikipedia_search, Title: Building a RAG chain from Wikipedia | ApertureDB Content: 21355\n",
      "2024-12-20 05:25:04 [root] INFO: Splitting document into 59 chunks\n",
      "2024-12-20 05:25:05 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:25:05 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:25:05 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:25:05 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:06 [root] INFO: URL: https://docs.aperturedata.io/HowToGuides/Applications/semantic_search/cohere_wikipedia_ingest, Title: Ingesting Wikipedia into ApertureDB | ApertureDB Content: 5636\n",
      "2024-12-20 05:25:06 [root] INFO: Splitting document into 14 chunks\n",
      "2024-12-20 05:25:06 [aperturedb.ParallelQuery] INFO: Connection test successful with [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n",
      "2024-12-20 05:25:06 [aperturedb.ParallelLoader] INFO: Starting ingestion with batchsize=1000, numthreads=4\n",
      "2024-12-20 05:25:06 [aperturedb.ParallelQuery] INFO: Commands per query = 1, Blobs per query = 1\n",
      "2024-12-20 05:25:06 [aperturedb.ParallelQuery] INFO: Worker 0 executing 1 batches\n",
      "2024-12-20 05:25:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 16494,\n",
      " 'downloader/request_count': 53,\n",
      " 'downloader/request_method_count/GET': 53,\n",
      " 'downloader/response_bytes': 406903,\n",
      " 'downloader/response_count': 53,\n",
      " 'downloader/response_status_count/200': 52,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'dupefilter/filtered': 1301,\n",
      " 'elapsed_time_seconds': 41.85842,\n",
      " 'finish_reason': 'closespider_itemcount',\n",
      " 'finish_time': datetime.datetime(2024, 12, 20, 5, 25, 8, 214720, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 1847304,\n",
      " 'httpcompression/response_count': 50,\n",
      " 'item_scraped_count': 29,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/INFO': 206,\n",
      " 'log_count/WARNING': 21,\n",
      " 'memusage/max': 228212736,\n",
      " 'memusage/startup': 228212736,\n",
      " 'offsite/domains': 39,\n",
      " 'offsite/filtered': 405,\n",
      " 'request_depth_max': 4,\n",
      " 'response_received_count': 52,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 53,\n",
      " 'scheduler/dequeued/memory': 53,\n",
      " 'scheduler/enqueued': 183,\n",
      " 'scheduler/enqueued/memory': 183,\n",
      " 'start_time': datetime.datetime(2024, 12, 20, 5, 24, 26, 356300, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-20 05:25:08 [scrapy.core.engine] INFO: Spider closed (closespider_itemcount)\n"
     ]
    }
   ],
   "source": [
    "# Delete the vector store before starting\n",
    "ApertureDB.delete_vectorstore(args.descriptorset)\n",
    "\n",
    "crawler.crawl(LangChainSpider)\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can list the vectorstores in our ApertureDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 05:25:08 [aperturedb.CommonLibrary] WARNING: Utils.create_connector is deprecated and will be removed in a future release. Use CommonLibrary.create_connector instead.\n",
      "2024-12-20 05:25:08 [aperturedb.CommonLibrary] INFO: Using active configuration 'rag_demo'\n",
      "2024-12-20 05:25:08 [aperturedb.CommonLibrary] INFO: Configuration: [ragdemo-3h5g50ie.farm0000.cloud.aperturedata.io:55555 as admin using TCP with SSL=True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_count': 368,\n",
       "  '_dimensions': 384,\n",
       "  '_engines': ['HNSW'],\n",
       "  '_metrics': ['CS'],\n",
       "  '_name': 'test',\n",
       "  '_uniqueid': '2.0.6820'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ApertureDB.list_vectorstores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Next you want to use this vectorstore to drive a RAG (Retrieval-Augmented Generation) chain.\n",
    "\n",
    "See [Building a RAG Chain from a Website](https://docs.aperturedata.io/HowToGuides/Applications/website_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further information\n",
    "\n",
    "* [LangChain vectorstore integration](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.aperturedb.ApertureDB.html)\n",
    "* [ApertureDB documentation website](https://docs.aperturedata.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
